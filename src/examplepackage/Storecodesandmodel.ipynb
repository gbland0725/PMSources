{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedicated-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cl\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import statsmodels.api as sm\n",
    "from pandas import ExcelWriter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "scheduled-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file and modify columns. Returns a dataframe\n",
    "def tofread_csv(rawdata):\n",
    "    #read csv file.\n",
    "    data = pd.read_csv(rawdata, error_bad_lines=False)\n",
    "    #rennaming 1st column as ID\n",
    "    data = data.rename(index=str, columns={\"Unnamed: 0\": \"ID\"})\n",
    "    #drop columns that have volume and size\n",
    "    data = data[data.columns.drop(list(data.filter(regex='vol')))]\n",
    "    data = data[data.columns.drop(list(data.filter(regex='size')))]\n",
    "    data.columns = data.columns.str.replace('_mass_g','')\n",
    "    return data\n",
    "\n",
    "#non_zero_data drops rows that has a zero value in every isotope returning rows that have particle events. Should be ran first before starting other functions!\n",
    "def non_zero_data(data):\n",
    "    #data = data.drop(columns = \"ID\")\n",
    "    non_zero_rows = data.abs().sum(axis=1) > 0.0\n",
    "    non_zero_data = data[non_zero_rows]\n",
    "    #non_zero_columns = non_zero_data.abs().sum(axis=0) > 0.0\n",
    "    #non_zero_data = non_zero_data.loc[: , non_zero_columns]\n",
    "    return non_zero_data\n",
    "\n",
    "#wrap the tofread function\n",
    "def wrapper(string, ELEMENTS = None):\n",
    "    df = pd.DataFrame()\n",
    "    #ELEMENTS = 4\n",
    "    for i in glob.glob(string):\n",
    "        df = pd.concat([tofread_csv(i), df], axis = 0, sort = False)\n",
    "    df.drop(columns = ['Index', r'timestamp /s'], inplace = True)\n",
    "    df.columns = df.columns.str.replace(r' /g', '')\n",
    "    df.columns = df.columns.str.replace(r'[', '')\n",
    "    df.columns = df.columns.str.replace('+', '')\n",
    "    df.columns = df.columns.str.replace(']', '')\n",
    "    if ELEMENTS != None:\n",
    "        df.drop(columns = ELEMENTS, inplace = True)\n",
    "    return non_zero_data(df)\n",
    "\n",
    "import copy\n",
    "\n",
    "#add noise to the data\n",
    "def addnoise(df, noise, coefficient = None):\n",
    "    if coefficient != None:\n",
    "        df = df*coefficient\n",
    "    df = df.fillna(0) + np.random.random(df.shape)*noise\n",
    "    return df\n",
    "\n",
    "#put multiple DFs together with the key\n",
    "def combineddf(dflist, dfkeys, ELEMENTS, noise = None, coefficient = None,):\n",
    "    #make an empty list\n",
    "    combinedlist = []\n",
    "    #make an empty dataframe\n",
    "    combineddf12 = pd.DataFrame()\n",
    "\n",
    "    for i,j in zip(dflist, dfkeys):\n",
    "        if noise == None and coefficient == None:\n",
    "            df = non_zero_data(i[ELEMENTS])\n",
    "            #fillnan with 0\n",
    "            df[np.isnan(df)] = 0\n",
    "        else:\n",
    "            #add noise\n",
    "            df = addnoise(i[ELEMENTS], noise, coefficient)\n",
    "\n",
    "\n",
    "        df['labels'] = j\n",
    "        combinedlist.append(df)\n",
    "        combineddf12 = pd.concat([combineddf12, df])\n",
    "    return combineddf12\n",
    "\n",
    "#return random projection codes for each sample\n",
    "def flyprojection(data, keys, ELEMENTS, expansionfactor):\n",
    "    data1 = combineddf(data, keys, ELEMENTS)\n",
    "    sourcenorm = copy.deepcopy(data1.iloc[:,:-1][ELEMENTS])\n",
    "\n",
    "    #had to center the mean like this because certain elements did not have any particle events for certain sources\n",
    "    for i in ELEMENTS:\n",
    "        sourcenorm[i] =sourcenorm[i].fillna(0)\n",
    "        try:\n",
    "            sourcenorm[i] = sourcenorm[i]/sourcenorm[i].mean(axis=0)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    sourcenorm = sourcenorm.fillna(0)  \n",
    "    codes1 = encode(np.array(sourcenorm), expansionfactor)\n",
    "    return codes1\n",
    "\n",
    "#apply the cluster labels with corresponding max probability\n",
    "def applylabels(data, keys, ELEMENTS, model, codes):\n",
    "    data1 = combineddf(data, keys, ELEMENTS)\n",
    "    labelstransform = model.transform(codes)\n",
    "    data1['clusters'] = labelstransform.argmax(axis = 1)\n",
    "    data1['clustersprob'] = labelstransform.max(axis = 1)\n",
    "    return data1, labelstransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "auburn-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "DROPELEMENTS = ['28Si', '29Si', '56Fe', '24Mg', '25Mg', '27Al']\n",
    "\n",
    "\n",
    "#heating\n",
    "Q16_12_10 = wrapper('sample_data/particle_masses_16_12_10**', DROPELEMENTS)\n",
    "Q16_12_11 = wrapper('sample_data/particle_masses_16_12_11**', DROPELEMENTS)\n",
    "Q16_12_12 = wrapper('sample_data/particle_masses_16_12_12**', DROPELEMENTS)\n",
    "Q16_12_13 = wrapper('sample_data/particle_masses_16_12_13**', DROPELEMENTS)\n",
    "Q16_12_14 = wrapper('sample_data/particle_masses_16_12_14**', DROPELEMENTS)\n",
    "Q16_12_15 = wrapper('sample_data/particle_masses_16_12_15**', DROPELEMENTS)\n",
    "Q16_12_16 = wrapper('sample_data/particle_masses_16_12_16**', DROPELEMENTS)\n",
    "Q16_12_17 = wrapper('sample_data/particle_masses_16_12_17**', DROPELEMENTS)\n",
    "Q16_12_18 = wrapper('sample_data/particle_masses_16_12_18**', DROPELEMENTS)\n",
    "Q16_12_19 = wrapper('sample_data/particle_masses_16_12_19**', DROPELEMENTS)\n",
    "Q16_12_20 = wrapper('sample_data/particle_masses_16_12_20**', DROPELEMENTS)\n",
    "Q16_12_21 = wrapper('sample_data/particle_masses_16_12_21**', DROPELEMENTS)\n",
    "Q16_12_22 = wrapper('sample_data/particle_masses_16_12_22**', DROPELEMENTS)\n",
    "\n",
    "Qian2016 = [Q16_12_10, Q16_12_11, Q16_12_12, Q16_12_13, Q16_12_14, Q16_12_15, Q16_12_16, Q16_12_17, Q16_12_18, Q16_12_19, Q16_12_20, Q16_12_21, Q16_12_22]\n",
    "Qian2016keys = ['Q16_12_10', 'Q16_12_11', 'Q16_12_12', 'Q16_12_13', 'Q16_12_14', 'Q16_12_15', 'Q16_12_16', 'Q16_12_17', 'Q16_12_18', 'Q16_12_19', 'Q16_12_20', 'Q16_12_21', 'Q16_12_22']\n",
    "\n",
    "\n",
    "#Nonheating\n",
    "Q18_03_06 = wrapper('sample_data/particle_masses_18_03_06**', DROPELEMENTS)\n",
    "Q18_03_07 = wrapper('sample_data/particle_masses_18_03_07**', DROPELEMENTS)\n",
    "Q18_03_08 = wrapper('sample_data/particle_masses_18_03_08**', DROPELEMENTS)\n",
    "Q18_03_09 = wrapper('sample_data/particle_masses_18_03_09**', DROPELEMENTS)\n",
    "Q18_03_10 = wrapper('sample_data/particle_masses_18_03_10**', DROPELEMENTS)\n",
    "Q18_03_11 = wrapper('sample_data/particle_masses_18_03_11**', DROPELEMENTS)\n",
    "Q18_03_12 = wrapper('sample_data/particle_masses_18_03_12**', DROPELEMENTS)\n",
    "Q18_03_13 = wrapper('sample_data/particle_masses_18_03_13**', DROPELEMENTS)\n",
    "Q18_03_14 = wrapper('sample_data/particle_masses_18_03_14**', DROPELEMENTS)\n",
    "Q18_03_15 = wrapper('sample_data/particle_masses_18_03_15**', DROPELEMENTS)\n",
    "\n",
    "Qian2018 = [Q18_03_06, Q18_03_07, Q18_03_08, Q18_03_09, Q18_03_10, Q18_03_11, Q18_03_12, Q18_03_13, Q18_03_14, Q18_03_15]\n",
    "Qian2018keys = ['Q18_03_06', 'Q18_03_07', 'Q18_03_08', 'Q18_03_09', 'Q18_03_10', 'Q18_03_11', 'Q18_03_12', 'Q18_03_13', 'Q18_03_14', 'Q18_03_15']\n",
    "\n",
    "\n",
    "#Urban\n",
    "QU18_12_02 = wrapper('sample_data/particle_masses_U18_12_02**', DROPELEMENTS)\n",
    "QU18_12_06 = wrapper('sample_data/particle_masses_U18_12_06**', DROPELEMENTS)\n",
    "QU18_12_13 = wrapper('sample_data/particle_masses_U18_12_13**', DROPELEMENTS)\n",
    "QU18_12_14 = wrapper('sample_data/particle_masses_U18_12_14**', DROPELEMENTS)\n",
    "QU18_12_18 = wrapper('sample_data/particle_masses_U18_12_18**', DROPELEMENTS)\n",
    "QU18_12_26 = wrapper('sample_data/particle_masses_U18_12_26**', DROPELEMENTS)\n",
    "QU18_12_28 = wrapper('sample_data/particle_masses_U18_12_28**', DROPELEMENTS)\n",
    "QU19_01_02 = wrapper('sample_data/particle_masses_U19_01_02**', DROPELEMENTS)\n",
    "\n",
    "QianU2019 = [QU18_12_02, QU18_12_06, QU18_12_13, QU18_12_14, QU18_12_18, QU18_12_26, QU18_12_28, QU19_01_02]\n",
    "QianU2019keys = ['QU18_12_02', 'QU18_12_06', 'QU18_12_13', 'QU18_12_14', 'QU18_12_18', 'QU18_12_26', 'QU18_12_28', 'QU19_01_02']\n",
    "\n",
    "\n",
    "#Rural\n",
    "QR18_12_02 = wrapper('sample_data/particle_masses_R18_12_02**', DROPELEMENTS)\n",
    "QR18_12_06 = wrapper('sample_data/particle_masses_R18_12_05**', DROPELEMENTS)\n",
    "QR18_12_13 = wrapper('sample_data/particle_masses_R18_12_13**', DROPELEMENTS)\n",
    "QR18_12_14 = wrapper('sample_data/particle_masses_R18_12_14**', DROPELEMENTS)\n",
    "QR18_12_18 = wrapper('sample_data/particle_masses_R18_12_18**', DROPELEMENTS)\n",
    "QR18_12_26 = wrapper('sample_data/particle_masses_R18_12_26**', DROPELEMENTS)\n",
    "QR18_12_28 = wrapper('sample_data/particle_masses_R18_12_28**', DROPELEMENTS)\n",
    "QR19_01_02 = wrapper('sample_data/particle_masses_R19_01_02**', DROPELEMENTS)\n",
    "\n",
    "QianR2019 = [QR18_12_02, QR18_12_06, QR18_12_13, QR18_12_14, QR18_12_18, QR18_12_26, QR18_12_28, QR19_01_02]\n",
    "QianR2019keys = ['QR18_12_02', 'QR18_12_06', 'QR18_12_13', 'QR18_12_14', 'QR18_12_18', 'QR18_12_26', 'QR18_12_28', 'QR19_01_02']\n",
    "\n",
    "\n",
    "#source samples\n",
    "soildust = wrapper('sample_data/particle_masses_Source1**', DROPELEMENTS)\n",
    "condust = wrapper('sample_data/particle_masses_Source2**', DROPELEMENTS)\n",
    "condsand = wrapper('sample_data/particle_masses_Source3**', DROPELEMENTS)\n",
    "coalburning = wrapper('sample_data/particle_masses_Source4**', DROPELEMENTS)\n",
    "indemission = wrapper('sample_data/particle_masses_Source5**', DROPELEMENTS)\n",
    "urbanfugdust = wrapper('sample_data/particle_masses_Source6**', DROPELEMENTS)\n",
    "\n",
    "Carexhaust = wrapper('sample_data/particle_masses_car**', DROPELEMENTS)\n",
    "biomass = wrapper('sample_data/particle_masses_biomass**', DROPELEMENTS)\n",
    "\n",
    "#drop duplicate columns\n",
    "Carexhaust = Carexhaust.loc[:,~Carexhaust.columns.duplicated()]\n",
    "biomass = biomass.loc[:,~biomass.columns.duplicated()]\n",
    "\n",
    "\n",
    "Sourcesamples = [soildust, condust, condsand, coalburning, indemission, urbanfugdust, Carexhaust, biomass]\n",
    "Sourcekeys = ['Soil dust', 'Construction dust', 'Construction sand', 'Coal burning', 'Industrial emission', 'Urban fugitive dust', 'Car exhaust', 'Biomass']\n",
    "\n",
    "\n",
    "#blanks\n",
    "blank1 = wrapper('sample_data/particle_masses_blank 1**', DROPELEMENTS)\n",
    "blank2 = wrapper('sample_data/particle_masses_blank 2**', DROPELEMENTS)\n",
    "blank3 = wrapper('sample_data/particle_masses_blank 3**', DROPELEMENTS)\n",
    "blank4 = wrapper('sample_data/particle_masses_blank 4**', DROPELEMENTS)\n",
    "blank5 = wrapper('sample_data/particle_masses_blank 5**', DROPELEMENTS)\n",
    "\n",
    "blank = [blank1, blank2, blank3, blank4, blank5]\n",
    "blankkeys = ['blank1', 'blank2', 'blank3', 'blank4', 'blank5']\n",
    "\n",
    "PPFilter1 = wrapper('sample_data/particle_masses_PP Filter 1**', DROPELEMENTS)\n",
    "PPFilter2 = wrapper('sample_data/particle_masses_PP Filter 2**', DROPELEMENTS)\n",
    "QuartzFilter1 = wrapper('sample_data/particle_masses_Quartz Filter 1**', DROPELEMENTS)\n",
    "QuartzFilter2 = wrapper('sample_data/particle_masses_Quartz Filter 2**', DROPELEMENTS)\n",
    "\n",
    "Filterblank = [PPFilter1, PPFilter2, QuartzFilter1, QuartzFilter2]\n",
    "Filterblankkeys = ['PPFilter1', 'PPFilter2', 'QuartzFilter1', 'QuartzFilter2']\n",
    "\n",
    "airkeys = [Qian2016keys, Qian2018keys, QianU2019keys, QianR2019keys, Sourcekeys]\n",
    "airsection = ['Heating', 'Non-Heating', 'Urban', 'Rural', 'Sources']\n",
    "airsamples = [Qian2016, Qian2018, QianU2019, QianR2019, Sourcesamples]\n",
    "\n",
    "\n",
    "ELEMENTS = ['48Ti', '52Cr', '55Mn', '54Fe', '59Co', '58Ni', '63Cu', '64Zn', '69Ga', '72Ge', '75As', '78Se', '85Rb', '88Sr', '89Y', '90Zr', '93Nb', \n",
    "           '98Mo', '108Pd', '107Ag', '114Cd', '120Sn', '121Sb', '130Te', '133Cs', '138Ba', '139La', '140Ce', '152Sm', '153Eu', '158Gd', '159Tb', '164Dy', '165Ho',\n",
    "           '166Er', '169Tm', '174Yb', '175Lu', '180Hf', '181Ta', '185Re', '192Os', '193Ir', '195Pt', '197Au', '202Hg', '208Pb', '238U', '51V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "shaped-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "d = len(ELEMENTS)  # number of features\n",
    "expansion_factor = 128\n",
    "m = expansion_factor * d  # dimension of the code space\n",
    "\n",
    "def top_idx(a, k):\n",
    "    \"\"\"indices of the top k values of each row of a\"\"\"\n",
    "    return np.argpartition(a, -k)[:, -k:]\n",
    "\n",
    "def top_mask(a, k):\n",
    "    \"\"\"create a boolean matrix the same shape as a indicating the top k items of each row\"\"\"\n",
    "    idx = top_idx(a, k)\n",
    "    mask = np.zeros_like(a)\n",
    "    row_idx = np.arange(len(a)).reshape(-1,1)\n",
    "    mask[np.repeat(row_idx, k, axis=-1), idx] = 1\n",
    "    return mask\n",
    "\n",
    "# from the supplementary materials:\n",
    "# A simple model of M is a sparse, binary random matrix:\n",
    "# each entry M_ij is set independently with probability p. Choosingp= 6/d, for instance,\n",
    "# would mean that each row of M has roughly 6 entries equal to 1 (and all of the other\n",
    "# entries are 0), which matches experimental findings.\n",
    "p = 8 / float(d)\n",
    "# M is the random mapping from features to codes (in the supplementary materials it is a m x d matrix),\n",
    "# here it is swapped so we can do data * M = codes where data is n samples x d and codes in n samples x m\n",
    "M = (rng.random(size=(d, m)) < p).astype(np.uint8)\n",
    "\n",
    "def encode(data, k):\n",
    "    \"\"\"Encode the data using random projection and winner take all approach \"\"\"\n",
    "    res = data @ M\n",
    "    mask = top_mask(res, k)\n",
    "    return res * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nominated-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9 # the k largest values of the code will be used as the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acknowledged-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sourcecodes = flyprojection(Sourcesamples, Sourcekeys, ELEMENTS, k)\n",
    "Qian2016codes = flyprojection(Qian2016, Qian2016keys, ELEMENTS, k)\n",
    "Qian2018codes = flyprojection(Qian2018, Qian2018keys, ELEMENTS, k)\n",
    "QianU2019codes = flyprojection(QianU2019, QianU2019keys, ELEMENTS, k)\n",
    "QianR2019codes = flyprojection(QianR2019, QianR2019keys, ELEMENTS, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accomplished-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save object\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-gates",
   "metadata": {},
   "source": [
    "# Fit and store Model and Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "primary-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "model2 = LatentDirichletAllocation(n_components=30)\n",
    "\n",
    "model2 = model2.fit(Sourcecodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "painful-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model \n",
    "save_object(model2, 'models/LDAModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "radio-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save codes\n",
    "save_object(Sourcecodes, 'models/Sourcecodes.pkl')\n",
    "save_object(Qian2016codes, 'models/Qian2016codes.pkl')\n",
    "save_object(Qian2018codes, 'models/Qian2018codes.pkl')\n",
    "save_object(QianU2019codes, 'models/QianU2019codes.pkl')\n",
    "save_object(QianR2019codes, 'models/QianR2019codes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "julian-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applylabels\n",
    "Qian2016, Qian2016prob =  applylabels(Qian2016, Qian2016keys, ELEMENTS, model2, Qian2016codes)\n",
    "Qian2018, Qian2018prob =  applylabels(Qian2018, Qian2018keys, ELEMENTS, model2, Qian2018codes)\n",
    "QianU2019, QianU2019prob =  applylabels(QianU2019, QianU2019keys, ELEMENTS, model2, QianU2019codes)\n",
    "QianR2019, QianR2019prob =  applylabels(QianR2019, QianR2019keys, ELEMENTS, model2, QianR2019codes)\n",
    "Sourcesamples, Sourcesamplesprob =  applylabels(Sourcesamples, Sourcekeys, ELEMENTS, model2, Sourcecodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-channels",
   "metadata": {},
   "source": [
    "# Store dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "completed-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object([Qian2016, Qian2016prob, Qian2016keys], 'models/Qian2016DF.pkl')\n",
    "save_object([Qian2018, Qian2018prob, Qian2018keys], 'models/Qian2018DF.pkl')\n",
    "save_object([QianU2019, QianU2019prob, QianU2019keys], 'models/QianU2019DF.pkl')\n",
    "save_object([QianR2019, QianR2019prob, QianR2019keys], 'models/QianR2019DF.pkl')\n",
    "save_object([Sourcesamples, Sourcesamplesprob, Sourcekeys], 'models/SourceDF.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-energy",
   "metadata": {},
   "source": [
    "# Get the transformed probability vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.transform(Sourcecodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
