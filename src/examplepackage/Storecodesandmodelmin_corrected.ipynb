{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\r\n",
    "import math\r\n",
    "import os\r\n",
    "import glob\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.colors as cl\r\n",
    "import matplotlib.ticker as mtick\r\n",
    "from matplotlib.gridspec import GridSpec\r\n",
    "import statsmodels.api as sm\r\n",
    "from pandas import ExcelWriter\r\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Read the csv file and modify columns. Returns a dataframe\r\n",
    "def tofread_csv(rawdata):\r\n",
    "    #read csv file\r\n",
    "    data = pd.read_csv(rawdata, error_bad_lines=False)\r\n",
    "    #rennaming 1st column as ID\r\n",
    "    data = data.rename(index=str, columns= {\"Unnamed: 0\": \"ID\"})\r\n",
    "    #drop colums that have volume and size\\n\",\r\n",
    "    data = data[data.columns.drop(list(data.filter(regex='vol')))]\r\n",
    "    data = data[data.columns.drop(list(data.filter(regex='size')))]\r\n",
    "    data.columns = data.columns.str.replace('_mass_g','')\r\n",
    "    return data\r\n",
    "\r\n",
    "#non_zero_data drops rows that has a zero value in every isotope returning rows that have particle events. Should be ran first before starting other functions!\r\n",
    "def non_zero_data(data):\r\n",
    "    #data = data.drop(columns = \"ID\"),\r\n",
    "    non_zero_rows = data.abs().sum(axis=1) > 0.0\r\n",
    "    non_zero_data = data[non_zero_rows]\r\n",
    "    #non_zero_columns = non_zero_data.abs().sum(axis=0) > 0.0\r\n",
    "    #non_zero_data = non_zero_data.loc[: , non_zero_columns]\r\n",
    "    return non_zero_data\r\n",
    "\r\n",
    "#wrap the tofread function\\n\",\r\n",
    "def wrapper(string, ELEMENTS = None):\r\n",
    "    df = pd.DataFrame()\r\n",
    "    #ELEMENTS = 4\\n\",\r\n",
    "    for i in glob.glob(string):\r\n",
    "        df = pd.concat([tofread_csv(i), df], axis = 0, sort = False)\r\n",
    "    df.drop(columns = ['Index', r'timestamp /s'], inplace = True)\r\n",
    "    df.columns = df.columns.str.replace(r' /g', '')\r\n",
    "    df.columns = df.columns.str.replace('[', '')\r\n",
    "    df.columns = df.columns.str.replace('+', '')\r\n",
    "    df.columns = df.columns.str.replace(']', '')\r\n",
    "    if ELEMENTS != None:\r\n",
    "        df.drop(columns = ELEMENTS, inplace = True)\r\n",
    "    return non_zero_data(df)\r\n",
    "\r\n",
    "import copy\r\n",
    "\r\n",
    "#add noise to the data\\n\",\r\n",
    "def addnoise(df, noise, coefficient = None):\r\n",
    "    if coefficient != None:\r\n",
    "        df = df*coefficient\r\n",
    "    df = df.fillna(0) + np.random.random(df.shape)*noise\r\n",
    "    return df\r\n",
    "\r\n",
    "#put multiple DFs together with the key\r\n",
    "def combineddf(dflist, dfkeys, ELEMENTS, noise = None, coefficient = None):\r\n",
    "    #make an empty list\r\n",
    "    combinedlist = []\r\n",
    "    #make an empty dataframe\r\n",
    "    combineddf12 = pd.DataFrame()\r\n",
    "    for i,j in zip(dflist, dfkeys):\r\n",
    "        if noise == None and coefficient == None:\r\n",
    "            df = non_zero_data(i[ELEMENTS])\r\n",
    "            #fillnan with 0\r\n",
    "            df[np.isnan(df)] = 0\r\n",
    "        else:\r\n",
    "            #add noise\r\n",
    "            df = addnoise(i[ELEMENTS], noise, coefficient)\r\n",
    "        df['labels'] = j\r\n",
    "        combinedlist.append(df)\r\n",
    "        combineddf12 = pd.concat([combineddf12, df])\r\n",
    "    return combineddf12\r\n",
    "\r\n",
    "#return random projection codes for each sample\r\n",
    "def flyprojection(data, keys, ELEMENTS, expansionfactor):\r\n",
    "    data1 = combineddf(data, keys, ELEMENTS)\r\n",
    "    sourcenorm = copy.deepcopy(data1.iloc[:,:-1][ELEMENTS])\r\n",
    "    #had to center the mean like this because certain elements did not have any particle events for certain sources\r\n",
    "    for i in ELEMENTS:\r\n",
    "        sourcenorm[i] =sourcenorm[i].fillna(0)\r\n",
    "        try:\r\n",
    "            sourcenorm[i] = sourcenorm[i]/sourcenorm[i].mean(axis=0)\r\n",
    "        except ValueError:\r\n",
    "            continue\r\n",
    "    sourcenorm = sourcenorm.fillna(0)\r\n",
    "    codes1 = encode(np.array(sourcenorm), expansionfactor)\r\n",
    "    return codes1\r\n",
    "\r\n",
    "#apply the cluster labels with corresponding max probability\r\n",
    "def applylabels(data, keys, ELEMENTS, model, codes):\r\n",
    "    data1 = combineddf(data, keys, ELEMENTS)\r\n",
    "    labelstransform = model.transform(codes)\r\n",
    "    data1['clusters'] = labelstransform.argmax(axis = 1)\r\n",
    "    data1['clustersprob'] = labelstransform.max(axis = 1)\r\n",
    "    return data1, labelstransform"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#import the data\r\n",
    "DROPELEMENTS = ['28Si', '29Si', '56Fe', '24Mg', '25Mg', '27Al']\r\n",
    "\r\n",
    "#heating\r\n",
    "Q16_12_10 = wrapper('sample_data/particle_masses_16_12_10**', DROPELEMENTS)\r\n",
    "Q16_12_11 = wrapper('sample_data/particle_masses_16_12_11**', DROPELEMENTS)\r\n",
    "Q16_12_12 = wrapper('sample_data/particle_masses_16_12_12**', DROPELEMENTS)\r\n",
    "Q16_12_13 = wrapper('sample_data/particle_masses_16_12_13**', DROPELEMENTS)\r\n",
    "Q16_12_14 = wrapper('sample_data/particle_masses_16_12_14**', DROPELEMENTS)\r\n",
    "Q16_12_15 = wrapper('sample_data/particle_masses_16_12_15**', DROPELEMENTS)\r\n",
    "Q16_12_16 = wrapper('sample_data/particle_masses_16_12_16**', DROPELEMENTS)\r\n",
    "Q16_12_17 = wrapper('sample_data/particle_masses_16_12_17**', DROPELEMENTS)\r\n",
    "Q16_12_18 = wrapper('sample_data/particle_masses_16_12_18**', DROPELEMENTS)\r\n",
    "Q16_12_19 = wrapper('sample_data/particle_masses_16_12_19**', DROPELEMENTS)\r\n",
    "Q16_12_20 = wrapper('sample_data/particle_masses_16_12_20**', DROPELEMENTS)\r\n",
    "Q16_12_21 = wrapper('sample_data/particle_masses_16_12_21**', DROPELEMENTS)\r\n",
    "Q16_12_22 = wrapper('sample_data/particle_masses_16_12_22**', DROPELEMENTS)\r\n",
    "\r\n",
    "Qian2016 = [Q16_12_10, Q16_12_11, Q16_12_12, Q16_12_13, Q16_12_14, Q16_12_15, Q16_12_16, Q16_12_17, Q16_12_18, Q16_12_19, Q16_12_20, Q16_12_21, Q16_12_22]\r\n",
    "Qian2016keys = ['Q16_12_10', 'Q16_12_11', 'Q16_12_12', 'Q16_12_13', 'Q16_12_14', 'Q16_12_15', 'Q16_12_16', 'Q16_12_17', 'Q16_12_18', 'Q16_12_19', 'Q16_12_20', 'Q16_12_21', 'Q16_12_22']\r\n",
    "\r\n",
    "\r\n",
    "#Nonheating\r\n",
    "Q18_03_06 = wrapper('sample_data/particle_masses_18_03_06**', DROPELEMENTS)\r\n",
    "Q18_03_07 = wrapper('sample_data/particle_masses_18_03_07**', DROPELEMENTS)\r\n",
    "Q18_03_08 = wrapper('sample_data/particle_masses_18_03_08**', DROPELEMENTS)\r\n",
    "Q18_03_09 = wrapper('sample_data/particle_masses_18_03_09**', DROPELEMENTS)\r\n",
    "Q18_03_10 = wrapper('sample_data/particle_masses_18_03_10**', DROPELEMENTS)\r\n",
    "Q18_03_11 = wrapper('sample_data/particle_masses_18_03_11**', DROPELEMENTS)\r\n",
    "Q18_03_12 = wrapper('sample_data/particle_masses_18_03_12**', DROPELEMENTS)\r\n",
    "Q18_03_13 = wrapper('sample_data/particle_masses_18_03_13**', DROPELEMENTS)\r\n",
    "Q18_03_14 = wrapper('sample_data/particle_masses_18_03_14**', DROPELEMENTS)\r\n",
    "Q18_03_15 = wrapper('sample_data/particle_masses_18_03_15**', DROPELEMENTS)\r\n",
    "\r\n",
    "Qian2018 = [Q18_03_06, Q18_03_07, Q18_03_08, Q18_03_09, Q18_03_10, Q18_03_11, Q18_03_12, Q18_03_13, Q18_03_14, Q18_03_15]\r\n",
    "Qian2018keys = ['Q18_03_06', 'Q18_03_07', 'Q18_03_08', 'Q18_03_09', 'Q18_03_10', 'Q18_03_11', 'Q18_03_12', 'Q18_03_13', 'Q18_03_14', 'Q18_03_15']\r\n",
    "\r\n",
    "\r\n",
    "#Urban\r\n",
    "QU18_12_02 = wrapper('sample_data/particle_masses_U18_12_02**', DROPELEMENTS)\r\n",
    "QU18_12_06 = wrapper('sample_data/particle_masses_U18_12_06**', DROPELEMENTS)\r\n",
    "QU18_12_13 = wrapper('sample_data/particle_masses_U18_12_13**', DROPELEMENTS)\r\n",
    "QU18_12_14 = wrapper('sample_data/particle_masses_U18_12_14**', DROPELEMENTS)\r\n",
    "QU18_12_18 = wrapper('sample_data/particle_masses_U18_12_18**', DROPELEMENTS)\r\n",
    "QU18_12_26 = wrapper('sample_data/particle_masses_U18_12_26**', DROPELEMENTS)\r\n",
    "QU18_12_28 = wrapper('sample_data/particle_masses_U18_12_28**', DROPELEMENTS)\r\n",
    "QU19_01_02 = wrapper('sample_data/particle_masses_U19_01_02**', DROPELEMENTS)\r\n",
    "\r\n",
    "QianU2019 = [QU18_12_02, QU18_12_06, QU18_12_13, QU18_12_14, QU18_12_18, QU18_12_26, QU18_12_28, QU19_01_02]\r\n",
    "QianU2019keys = ['QU18_12_02', 'QU18_12_06', 'QU18_12_13', 'QU18_12_14', 'QU18_12_18', 'QU18_12_26', 'QU18_12_28', 'QU19_01_02']\r\n",
    "\r\n",
    "\r\n",
    "#Rural\r\n",
    "QR18_12_02 = wrapper('sample_data/particle_masses_R18_12_02**', DROPELEMENTS)\r\n",
    "QR18_12_06 = wrapper('sample_data/particle_masses_R18_12_05**', DROPELEMENTS)\r\n",
    "QR18_12_13 = wrapper('sample_data/particle_masses_R18_12_13**', DROPELEMENTS)\r\n",
    "QR18_12_14 = wrapper('sample_data/particle_masses_R18_12_14**', DROPELEMENTS)\r\n",
    "QR18_12_18 = wrapper('sample_data/particle_masses_R18_12_18**', DROPELEMENTS)\r\n",
    "QR18_12_26 = wrapper('sample_data/particle_masses_R18_12_26**', DROPELEMENTS)\r\n",
    "QR18_12_28 = wrapper('sample_data/particle_masses_R18_12_28**', DROPELEMENTS)\r\n",
    "QR19_01_02 = wrapper('sample_data/particle_masses_R19_01_02**', DROPELEMENTS)\r\n",
    "\r\n",
    "QianR2019 = [QR18_12_02, QR18_12_06, QR18_12_13, QR18_12_14, QR18_12_18, QR18_12_26, QR18_12_28, QR19_01_02]\r\n",
    "QianR2019keys = ['QR18_12_02', 'QR18_12_06', 'QR18_12_13', 'QR18_12_14', 'QR18_12_18', 'QR18_12_26', 'QR18_12_28', 'QR19_01_02']\r\n",
    "\r\n",
    "\r\n",
    "#source samples\r\n",
    "soildust = wrapper('sample_data/particle_masses_Source1**', DROPELEMENTS)\r\n",
    "condust = wrapper('sample_data/particle_masses_Source2**', DROPELEMENTS)\r\n",
    "condsand = wrapper('sample_data/particle_masses_Source3**', DROPELEMENTS)\r\n",
    "coalburning = wrapper('sample_data/particle_masses_Source4**', DROPELEMENTS)\r\n",
    "indemission = wrapper('sample_data/particle_masses_Source5**', DROPELEMENTS)\r\n",
    "urbanfugdust = wrapper('sample_data/particle_masses_Source6**', DROPELEMENTS)\r\n",
    "\r\n",
    "Carexhaust = wrapper('sample_data/particle_masses_car**', DROPELEMENTS)\r\n",
    "biomass = wrapper('sample_data/particle_masses_biomass**', DROPELEMENTS)\r\n",
    "\r\n",
    "#drop duplicate columns\r\n",
    "Carexhaust = Carexhaust.loc[:,~Carexhaust.columns.duplicated()]\r\n",
    "biomass = biomass.loc[:,~biomass.columns.duplicated()]\r\n",
    "\r\n",
    "\r\n",
    "Sourcesamples = [soildust, condust, condsand, coalburning, indemission, Carexhaust, biomass]\r\n",
    "Sourcekeys = ['Soil dust', 'Construction dust', 'Construction sand', 'Coal burning', 'Industrial emission', 'Car exhaust', 'Biomass']\r\n",
    "\r\n",
    "\r\n",
    "#blanks\r\n",
    "blank1 = wrapper('sample_data/particle_masses_blank 1**', DROPELEMENTS)\r\n",
    "blank2 = wrapper('sample_data/particle_masses_blank 2**', DROPELEMENTS)\r\n",
    "blank3 = wrapper('sample_data/particle_masses_blank 3**', DROPELEMENTS)\r\n",
    "blank4 = wrapper('sample_data/particle_masses_blank 4**', DROPELEMENTS)\r\n",
    "blank5 = wrapper('sample_data/particle_masses_blank 5**', DROPELEMENTS)\r\n",
    "\r\n",
    "blank = [blank1, blank2, blank3, blank4, blank5]\r\n",
    "blankkeys = ['blank1', 'blank2', 'blank3', 'blank4', 'blank5']\r\n",
    "\r\n",
    "PPFilter1 = wrapper('sample_data/particle_masses_PP Filter 1**', DROPELEMENTS)\r\n",
    "PPFilter2 = wrapper('sample_data/particle_masses_PP Filter 2**', DROPELEMENTS)\r\n",
    "QuartzFilter1 = wrapper('sample_data/particle_masses_Quartz Filter 1**', DROPELEMENTS)\r\n",
    "QuartzFilter2 = wrapper('sample_data/particle_masses_Quartz Filter 2**', DROPELEMENTS)\r\n",
    "\r\n",
    "Filterblank = [PPFilter1, PPFilter2, QuartzFilter1, QuartzFilter2]\r\n",
    "Filterblankkeys = ['PPFilter1', 'PPFilter2', 'QuartzFilter1', 'QuartzFilter2']\r\n",
    "\r\n",
    "airkeys = [Qian2016keys, Qian2018keys, QianU2019keys, QianR2019keys, Sourcekeys]\r\n",
    "airsection = ['Heating', 'Non-Heating', 'Urban', 'Rural', 'Sources']\r\n",
    "airsamples = [Qian2016, Qian2018, QianU2019, QianR2019, Sourcesamples]\r\n",
    "\r\n",
    "\r\n",
    "ELEMENTS = ['48Ti', '53Cr', '55Mn', '54Fe', '59Co', '58Ni', '63Cu', '64Zn', '69Ga', '72Ge', '75As', '78Se', '85Rb', '88Sr', '89Y', '90Zr', '93Nb',\r\n",
    "           '98Mo', '108Pd', '107Ag', '114Cd', '120Sn', '121Sb', '130Te', '133Cs', '138Ba', '139La', '140Ce', '152Sm', '153Eu', '158Gd', '159Tb', '164Dy', '165Ho',\r\n",
    "           '166Er', '169Tm', '174Yb', '175Lu', '180Hf', '185Re', '192Os', '193Ir', '195Pt', '197Au', '202Hg', '208Pb', '238U', '51V']\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#set each to the specific ELEMENTS\r\n",
    "soildust = non_zero_data(soildust[ELEMENTS])\r\n",
    "condust = non_zero_data(condust[ELEMENTS])\r\n",
    "condsand = non_zero_data(condsand[ELEMENTS])\r\n",
    "coalburning = non_zero_data(coalburning[ELEMENTS])\r\n",
    "indemission = non_zero_data(indemission[ELEMENTS])\r\n",
    "urbanfugdust = non_zero_data(urbanfugdust[ELEMENTS])\r\n",
    "Carexhaust = non_zero_data(Carexhaust[ELEMENTS])\r\n",
    "biomass = non_zero_data(biomass[ELEMENTS])\r\n",
    "\r\n",
    "#reset all the indices\r\n",
    "soildust.reset_index(inplace = True, drop = True)\r\n",
    "condust.reset_index(inplace = True, drop = True)\r\n",
    "condsand.reset_index(inplace = True, drop = True)\r\n",
    "coalburning.reset_index(inplace = True, drop = True)\r\n",
    "indemission.reset_index(inplace = True, drop = True)\r\n",
    "urbanfugdust.reset_index(inplace = True, drop = True)\r\n",
    "Carexhaust.reset_index(inplace = True, drop = True)\r\n",
    "biomass.reset_index(inplace = True, drop = True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#merge soil and condust and consand altogether\r\n",
    "\r\n",
    "soilcon = pd.concat([soildust, condust, condsand])\r\n",
    "\r\n",
    "Sourcesamples = [soilcon, coalburning, indemission, Carexhaust, biomass]\r\n",
    "Sourcekeys = ['Soil and Construction', 'Coal burning', 'Industrial emission', 'Car exhaust', 'Biomass']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#create a list with same amount of particles for each\r\n",
    "Sourcesamplessame = []\r\n",
    "for i in Sourcesamples:\r\n",
    "    Sourcesamplessame.append(i.sample(np.array([len(i) for i in Sourcesamples]).min(), random_state = 1))\r\n",
    "\r\n",
    "#reset the index\r\n",
    "for i in Sourcesamplessame:\r\n",
    "    i.reset_index(inplace = True, drop = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def split_data(data, percent):\r\n",
    "    data_percent = data.iloc[data.sample(frac = percent).index]\r\n",
    "    data_rest = data.loc[~data.index.isin(data_percent.index)]\r\n",
    "    return data_percent, data_rest\r\n",
    "\r\n",
    "Sourcesamples_training = []\r\n",
    "Sourcesamples_testing = []\r\n",
    "\r\n",
    "for i in Sourcesamplessame:\r\n",
    "    train, test = split_data(i, 0.8)\r\n",
    "    Sourcesamples_training.append(train)\r\n",
    "    Sourcesamples_testing.append(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "rng = np.random.default_rng(seed = 1000)\r\n",
    "\r\n",
    "d = len(ELEMENTS)  # number of features\r\n",
    "expansion_factor = 128\r\n",
    "m = expansion_factor * d  # dimension of the code space\r\n",
    "def top_idx(a, k):\r\n",
    "    #indices of the top k values of each row of a\r\n",
    "    return np.argpartition(a, -k)[:, -k:]\r\n",
    "\r\n",
    "def top_mask(a, k):\r\n",
    "   #create a boolean matrix the same shape as a indicating the top k items of each row\r\n",
    "    idx = top_idx(a, k)\r\n",
    "    mask = np.zeros_like(a)\r\n",
    "    row_idx = np.arange(len(a)).reshape(-1,1)\r\n",
    "    mask[np.repeat(row_idx, k, axis=-1), idx] = 1\r\n",
    "    return mask\r\n",
    "\r\n",
    "# from the supplementary materials:\r\n",
    "# A simple model of M is a sparse, binary random matrix:\r\n",
    "# each entry M_ij is set independently with probability p. Choosingp= 6/d, for instance\r\n",
    "# would mean that each row of M has roughly 6 entries equal to 1 (and all of the other\r\n",
    "# entries are 0), which matches experimental findings.\r\n",
    "p = 8 / float(d)\r\n",
    "# M is the random mapping from features to codes (in the supplementary materials it is a m x d matrix)\r\n",
    "# here it is swapped so we can do data * M = codes where data is n samples x d and codes in n samples x \r\n",
    "M = (rng.random(size=(d, m)) < p).astype(np.uint8)\r\n",
    "k = 9 # the k largest values of the code will be used as the result\r\n",
    "\r\n",
    "def encode(data, k):\r\n",
    "    #Encode the data using random projection and winner take all approach\r\n",
    "    res = data @ M\r\n",
    "    mask = top_mask(res, k)\r\n",
    "    return res * mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "Sourcecodes = flyprojection(Sourcesamples, Sourcekeys, ELEMENTS, k)\r\n",
    "Qian2016codes = flyprojection(Qian2016, Qian2016keys, ELEMENTS, k)\r\n",
    "Qian2018codes = flyprojection(Qian2018, Qian2018keys, ELEMENTS, k)\r\n",
    "QianU2019codes = flyprojection(QianU2019, QianU2019keys, ELEMENTS, k)\r\n",
    "QianR2019codes = flyprojection(QianR2019, QianR2019keys, ELEMENTS, k)\r\n",
    "\r\n",
    "#randomly project training and testing set\r\n",
    "Sourcesamples_trainingcodes = flyprojection(Sourcesamples_training, Sourcekeys, ELEMENTS, k)\r\n",
    "Sourcesamples_testingcodes = flyprojection(Sourcesamples_testing, Sourcekeys, ELEMENTS, k)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.72 MiB for an array with shape (137352, 9) and data type int32",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0d18cde390cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSourcecodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflyprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSourcesamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSourcekeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELEMENTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mQian2016codes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflyprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQian2016\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQian2016keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELEMENTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mQian2018codes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflyprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQian2018\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQian2018keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELEMENTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mQianU2019codes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflyprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQianU2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQianU2019keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELEMENTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mQianR2019codes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflyprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQianR2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQianR2019keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELEMENTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b47a24fd4ad7>\u001b[0m in \u001b[0;36mflyprojection\u001b[1;34m(data, keys, ELEMENTS, expansionfactor)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0msourcenorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msourcenorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mcodes1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msourcenorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansionfactor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodes1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-2b8a85671958>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(data, k)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#Encode the data using random projection and winner take all approach\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-2b8a85671958>\u001b[0m in \u001b[0;36mtop_mask\u001b[1;34m(a, k)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mrow_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tof3_7\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(a, repeats, axis)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \"\"\"\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'repeat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tof3_7\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.72 MiB for an array with shape (137352, 9) and data type int32"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#make an urbfugdust codes\r\n",
    "urbfugdustcodes = flyprojection([urbanfugdust], ['Urban fugitive dust'], ELEMENTS, k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#save object\r\n",
    "def save_object(obj, filename):\r\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\r\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit and store Model and Codes\r\n",
    "#fit the Sourcecodes_training\r\n",
    "from sklearn.decomposition import LatentDirichletAllocation\r\n",
    "\r\n",
    "model2 = LatentDirichletAllocation(n_components=20, doc_topic_prior = 0.01)\r\n",
    "\r\n",
    "model2 = model2.fit(Sourcesamples_trainingcodes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#save model\r\n",
    "save_object(model2, 'models/revision_3/LDAModel_4.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#save codes\r\n",
    "#save_object(Sourcecodes, 'models/revision_3/Sourcecodes_4.pkl'),\r\n",
    "save_object(Sourcesamples_trainingcodes, 'models/multipleLDAmodels/Sourcesamples_trainingcodes_5.pkl')\r\n",
    "save_object(Sourcesamples_testingcodes, 'models/multipleLDAmodels/Sourcesamples_testingcodes_5.pkl')\r\n",
    "save_object(Qian2016codes, 'models/multipleLDAmodels/Qian2016codes_5.pkl')\r\n",
    "save_object(Qian2018codes, 'models/multipleLDAmodels/Qian2018codes_5.pkl')\r\n",
    "save_object(QianU2019codes, 'models/multipleLDAmodels/QianU2019codes_5.pkl')\r\n",
    "save_object(QianR2019codes, 'models/multipleLDAmodels/QianR2019codes_5.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#applylabels\r\n",
    "Qian2016, Qian2016prob =  applylabels(Qian2016, Qian2016keys, ELEMENTS, model2, Qian2016codes)\r\n",
    "Qian2018, Qian2018prob =  applylabels(Qian2018, Qian2018keys, ELEMENTS, model2, Qian2018codes)\r\n",
    "QianU2019, QianU2019prob =  applylabels(QianU2019, QianU2019keys, ELEMENTS, model2, QianU2019codes)\r\n",
    "QianR2019, QianR2019prob =  applylabels(QianR2019, QianR2019keys, ELEMENTS, model2, QianR2019codes)\r\n",
    "Sourcesamples, Sourcesamplesprob =  applylabels(Sourcesamples, Sourcekeys, ELEMENTS, model2, Sourcecodes)\r\n",
    "Sourcesamples_training, Sourcesamplesprob_training = applylabels(Sourcesamples_training, Sourcekeys, ELEMENTS, model2, Sourcesamples_trainingcodes)\r\n",
    "Sourcesamples_testing, Sourcesamplesprob_testing = applylabels(Sourcesamples_testing, Sourcekeys, ELEMENTS, model2, Sourcesamples_testingcodes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#add urban fugitive dust\r\n",
    "Urbfugdust, Urbfugdustprob = applylabels([urbanfugdust], ['Urban fugitive dust'], ELEMENTS, model2, urbfugdustcodes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "save_object([Qian2016, Qian2016prob, Qian2016keys], 'models/revision_3/Qian2016DF_4.pkl')\r\n",
    "save_object([Qian2018, Qian2018prob, Qian2018keys], 'models/revision_3/Qian2018DF_4.pkl')\r\n",
    "save_object([QianU2019, QianU2019prob, QianU2019keys], 'models/revision_3/QianU2019DF_4.pkl')\r\n",
    "save_object([QianR2019, QianR2019prob, QianR2019keys], 'models/revision_3/QianR2019DF_4.pkl')\r\n",
    "save_object([Sourcesamples, Sourcesamplesprob, Sourcekeys], 'models/revision_3/SourceDF_4.pkl')\r\n",
    "save_object([Sourcesamples_training, Sourcesamplesprob_training, Sourcekeys], 'models/revision_3/SourcetrainingDF_4.pkl')\r\n",
    "save_object([Sourcesamples_testing, Sourcesamplesprob_testing, Sourcekeys], 'models/revision_3/SourcetestingDF_4.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#just the df\\r\\n\",\r\n",
    "save_object([Qian2016, Qian2016keys], 'models/multipleLDAmodels/Qian2016DF_5.pkl')\r\n",
    "save_object([Qian2018, Qian2018keys], 'models/multipleLDAmodels/Qian2018DF_5.pkl')\r\n",
    "save_object([QianU2019, QianU2019keys], 'models/multipleLDAmodels/QianU2019DF_5.pkl')\r\n",
    "save_object([QianR2019, QianR2019keys], 'models/multipleLDAmodels/QianR2019DF_5.pkl')\r\n",
    "save_object([Sourcesamples_training, Sourcekeys], 'models/multipleLDAmodels/SourcetrainingDF_5.pkl')\r\n",
    "save_object([Sourcesamples_testing, Sourcekeys], 'models/multipleLDAmodels/SourcetestingDF_5.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#add urban fugitive dust\r\n",
    "save_object([Urbfugdust, Urbfugdustprob, ['Urban fugitive dust']], 'models/revision_3/UrbanfugdustDF_4.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "isotope_matrix = model2.components_ @ np.transpose(M)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "isotope_DF = pd.DataFrame(isotope_matrix, columns = ELEMENTS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.bar(isotope_DF.columns, isotope_DF.loc[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "element_ranks_list = []\r\n",
    "for i in np.arange(0, 30, 1):\r\n",
    "    element_ranks_list.append(isotope_DF.loc[i].sort_values(ascending = False)[0:10].index)\r\n",
    "\r\n",
    "pd.DataFrame(element_ranks_list).to_excel('models/revision_2/ranked_elements.xlsx')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LDA with different parameters\r\n",
    "import itertools\r\n",
    "#fit the Sourcecodes_training\r\n",
    "from sklearn.decomposition import LatentDirichletAllocation\r\n",
    "\r\n",
    "\r\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 0.9]\r\n",
    "number_of_clusters = [5, 10, 15, 30, 50]\r\n",
    "\r\n",
    "for j,i in enumerate(itertools.product(alpha, number_of_clusters)):\r\n",
    "    model2 = LatentDirichletAllocation(n_components=i[1], doc_topic_prior = i[0])\r\n",
    "\r\n",
    "    model2 = model2.fit(Sourcesamples_trainingcodes)\r\n",
    "    #save model \\r\\n\r\n",
    "    save_object([i, model2], 'models/multipleLDAmodels/LDAModelvariant_' + str(j) + '.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('tof3_7': conda)"
  },
  "interpreter": {
   "hash": "cbc87b2860f8814b7de3025acfb0f8acf56f05d2cf1676a29c9434e58249d92f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}